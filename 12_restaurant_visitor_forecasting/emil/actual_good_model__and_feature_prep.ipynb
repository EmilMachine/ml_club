{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this notebook:\n",
    "- https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf8\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "#from com_util import *\n",
    "import gc\n",
    "import os\n",
    "\n",
    "##############################################\n",
    "#encoding=utf8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def encode_onehot(df,column_name):\n",
    "    feature_df=pd.get_dummies(df[column_name], prefix=column_name)\n",
    "    all = pd.concat([df.drop([column_name], axis=1),feature_df], axis=1)\n",
    "    return all\n",
    "\n",
    "def encode_count(df,column_name):\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df[column_name].values))\n",
    "    df[column_name] = lbl.transform(list(df[column_name].values))\n",
    "    return df\n",
    "\n",
    "def merge_count(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].count()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_nunique(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].nunique()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_median(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].median()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_mean(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].mean()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_sum(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].sum()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_max(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].max()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_min(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].min()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_std(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].std()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def merge_var(df,columns,value,cname):\n",
    "    add = pd.DataFrame(df.groupby(columns)[value].var()).reset_index()\n",
    "    add.columns=columns+[cname]\n",
    "    df=df.merge(add,on=columns,how=\"left\")\n",
    "    return df\n",
    "\n",
    "def feat_count(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].count()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_count\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_nunique(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].nunique()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_nunique\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_mean(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].mean()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_mean\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_kernelMedian(df, df_feature, fe, value, pr, name=\"\"):\n",
    "    def get_median(a, pr=pr):\n",
    "        a = np.array(a)\n",
    "        x = a[~np.isnan(a)]\n",
    "        n = len(x)\n",
    "        weight = np.repeat(1.0, n)\n",
    "        idx = np.argsort(x)\n",
    "        x = x[idx]\n",
    "        if n<pr.shape[0]:\n",
    "            pr = pr[n,:n]\n",
    "        else:\n",
    "            scale = (n-1)/2.\n",
    "            xxx = np.arange(-(n+1)/2.+1, (n+1)/2., step=1)/scale\n",
    "            yyy = 3./4.*(1-xxx**2)\n",
    "            yyy = yyy/np.sum(yyy)\n",
    "            pr = (yyy*n+1)/(n+1)\n",
    "        ans = np.sum(pr*x*weight) / float(np.sum(pr * weight))\n",
    "        return ans\n",
    "\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].apply(get_median)).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_mean\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_std(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].std()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_std\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_median(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].median()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_median\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_max(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].max()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_max\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_min(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].min()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_min\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_sum(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].sum()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_sum\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_var(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].var()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_var\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_quantile(df, df_feature, fe,value,n,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].quantile(n)).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_quantile\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/air_visit_data.csv')\n",
    "df_train[\"visitors\"] = df_train[\"visitors\"].apply(lambda x: np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "df_test = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "store_id_relation = pd.read_csv(\"./data/store_id_relation.csv\")\n",
    "\n",
    "df_test[\"air_store_id\"]=df_test[\"id\"].apply(lambda x:\"_\".join(x.split(\"_\")[:2]))\n",
    "df_test[\"visit_date\"]=df_test[\"id\"].apply(lambda x:x.split(\"_\")[2])\n",
    "del df_test[\"visitors\"]\n",
    "\n",
    "air_reserve = pd.read_csv(\"./data/air_reserve.csv\")\n",
    "hpg_reserve = pd.read_csv(\"./data/hpg_reserve.csv\")\n",
    "hpg_reserve = pd.merge(hpg_reserve, store_id_relation, how='inner', on=['hpg_store_id'])\n",
    "air_reserve[\"reserve_date\"]=air_reserve[\"reserve_datetime\"].apply(lambda x:x.split(\" \")[0])\n",
    "air_reserve[\"visit_date\"]=air_reserve[\"visit_datetime\"].apply(lambda x:x.split(\" \")[0])\n",
    "hpg_reserve[\"reserve_date\"]=hpg_reserve[\"reserve_datetime\"].apply(lambda x:x.split(\" \")[0])\n",
    "hpg_reserve[\"visit_date\"]=hpg_reserve[\"visit_datetime\"].apply(lambda x:x.split(\" \")[0])\n",
    "air_reserve['reserve_datetime_diff'] = (pd.to_datetime(air_reserve['visit_date']) - pd.to_datetime(air_reserve['reserve_date'])).dt.days\n",
    "hpg_reserve['reserve_datetime_diff'] = (pd.to_datetime(hpg_reserve['visit_date']) - pd.to_datetime(hpg_reserve['reserve_date'])).dt.days\n",
    "\n",
    "\n",
    "df_train=df_train.merge(store_id_relation,on=[\"air_store_id\"],how=\"left\")\n",
    "\n",
    "df_date_info= pd.read_csv(\"./data/date_info.csv\")\n",
    "df_date_info.columns=[\"visit_date\",\"day_of_week\",\"holiday_flg\"]\n",
    "df_date_info[\"day_of_week\"]=df_date_info[\"day_of_week\"].replace({\"Monday\":1,\"Tuesday\":2,\"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6,\"Sunday\":7})\n",
    "df_date_info[\"holiday\"] = ((df_date_info[\"day_of_week\"]>=6) | (df_date_info[\"holiday_flg\"]==1)).astype(int)\n",
    "#df_date_info[\"holiday\"]= map(lambda a, b: 1 if a in [6, 7] or b == 1 else 0, df_date_info[\"day_of_week\"], df_date_info[\"holiday_flg\"])\n",
    "del df_date_info[\"day_of_week\"]\n",
    "\n",
    "air_info=pd.read_csv(\"./data/air_store_info.csv\")\n",
    "air_info=encode_count(air_info,\"air_genre_name\")\n",
    "air_info=encode_count(air_info,\"air_area_name\")\n",
    "hpg_info=pd.read_csv(\"./data/hpg_store_info.csv\")\n",
    "hpg_info=encode_count(hpg_info,\"hpg_genre_name\")\n",
    "hpg_info=encode_count(hpg_info,\"hpg_area_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual feature engineering\n",
    "\n",
    "# 1\n",
    "PrOriginalEp = np.zeros((2000,2000))\n",
    "PrOriginalEp[1,0] = 1\n",
    "PrOriginalEp[2,range(2)] = [0.5,0.5]\n",
    "for i in range(3,2000):\n",
    "    scale = (i-1)/2.\n",
    "    x = np.arange(-(i+1)/2.+1, (i+1)/2., step=1)/scale\n",
    "    y = 3./4.*(1-x**2)\n",
    "    y = y/np.sum(y)\n",
    "    PrOriginalEp[i, range(i)] = y\n",
    "PrEp = PrOriginalEp.copy()\n",
    "for i in range(3, 2000):\n",
    "    PrEp[i,:i] = (PrEp[i,:i]*i+1)/(i+1)\n",
    "\n",
    "def date_gap(x,y):\n",
    "    a,b,c=x.split(\"-\")\n",
    "    return (date(int(a),int(b),int(c))-y).days\n",
    "\n",
    "def date_handle(df):\n",
    "    df_visit_date=pd.to_datetime(df[\"visit_date\"])\n",
    "    df[\"weekday\"]=df_visit_date.dt.weekday\n",
    "    df[\"day\"]=df_visit_date.dt.day\n",
    "    days_of_months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    # min days to reservations\n",
    "    df[\"days_to_side\"] = df_visit_date.apply(\n",
    "        lambda x: min(x.day, days_of_months[x.month-1]-x.day))\n",
    "    # days. \n",
    "    df[\"day\"]=df[\"day\"].apply(lambda x:0 if x<=7 else 2 if x>=24 else 1)\n",
    "    df = df.merge(air_info, on=\"air_store_id\", how=\"left\").fillna(-1)\n",
    "    #df = df.merge(hpg_info, on=\"hpg_store_id\", how=\"left\").fillna(-1)\n",
    "    df = df.merge(df_date_info, on=\"visit_date\", how=\"left\").fillna(-1)\n",
    "    #df[\"holiday\"] = map(lambda a, b: 1 if a in [5, 6] or b == 1 else 0, df[\"weekday\"], df[\"holiday_flg\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df_label,df_train,df_air_reserve,df_hpg_reserve):\n",
    "    df_train=date_handle(df_train)\n",
    "    df_label=date_handle(df_label)\n",
    "\n",
    "\n",
    "    #预定信息\n",
    "    # Feiyang: 4. 把这两段的 mean 改成了 kernelMedian\n",
    "    df_label=feat_sum(df_label,df_air_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_datetime_diff\",\"air_reserve_datetime_diff_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_air_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_datetime_diff\",PrEp,\"air_reserve_datetime_diff_mean\")\n",
    "    df_label=feat_sum(df_label,df_air_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",\"air_reserve_visitors_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_air_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",PrEp,\"air_reserve_visitors_mean\")\n",
    "    df_label=feat_sum(df_label,df_air_reserve,[\"visit_date\"],\"reserve_visitors\",\"air_date_reserve_visitors_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_air_reserve,[\"visit_date\"],\"reserve_visitors\",PrEp,\"air_date_reserve_visitors_mean\")\n",
    "\n",
    "    df_label=feat_sum(df_label,df_hpg_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_datetime_diff\",\"hpg_reserve_datetime_diff_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_hpg_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_datetime_diff\",PrEp,\"hpg_reserve_datetime_diff_mean\")\n",
    "    df_label=feat_sum(df_label,df_hpg_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",\"hpg_reserve_visitors_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_hpg_reserve,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",PrEp,\"hpg_reserve_visitors_mean\")\n",
    "    df_label=feat_sum(df_label,df_hpg_reserve,[\"visit_date\"],\"reserve_visitors\",\"hpg_date_reserve_visitors_sum\")\n",
    "    df_label=feat_kernelMedian(df_label,df_hpg_reserve,[\"visit_date\"],\"reserve_visitors\",PrEp,\"hpg_date_reserve_visitors_mean\")\n",
    "\n",
    "    for i in [35,63,140]:\n",
    "        df_air_reserve_select=df_air_reserve[df_air_reserve.day_gap>=-i].copy()\n",
    "        df_hpg_reserve_select=df_hpg_reserve[df_hpg_reserve.day_gap>=-i].copy()\n",
    "\n",
    "        # Feiyang: 5. 把这两段的 mean 改成了 kernelMedian\n",
    "        date_air_reserve=pd.DataFrame(df_air_reserve_select.groupby([\"air_store_id\",\"visit_date\"]).reserve_visitors.sum()).reset_index()\n",
    "        date_air_reserve.columns=[\"air_store_id\",\"visit_date\",\"reserve_visitors_sum\"]\n",
    "        date_air_reserve=feat_count(date_air_reserve,df_air_reserve_select,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",\"reserve_visitors_count\")\n",
    "        date_air_reserve=feat_kernelMedian(date_air_reserve,df_air_reserve_select,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",PrEp,\"reserve_visitors_mean\")\n",
    "\n",
    "        date_hpg_reserve=pd.DataFrame(df_hpg_reserve_select.groupby([\"air_store_id\",\"visit_date\"]).reserve_visitors.sum()).reset_index()\n",
    "        date_hpg_reserve.columns=[\"air_store_id\",\"visit_date\",\"reserve_visitors_sum\"]\n",
    "        date_hpg_reserve=feat_count(date_hpg_reserve,df_hpg_reserve_select,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",\"reserve_visitors_count\")\n",
    "        date_hpg_reserve=feat_kernelMedian(date_hpg_reserve,df_hpg_reserve_select,[\"air_store_id\",\"visit_date\"],\"reserve_visitors\",PrEp,\"reserve_visitors_mean\")\n",
    "\n",
    "        date_air_reserve=date_handle(date_air_reserve)\n",
    "        date_hpg_reserve=date_handle(date_hpg_reserve)\n",
    "        date_air_reserve[\"holiday\"] = ((date_air_reserve[\"weekday\"]>=5) | (date_air_reserve[\"holiday_flg\"]==1)).astype(int)\n",
    "        date_hpg_reserve[\"holiday\"] = ((date_hpg_reserve[\"weekday\"]>=5) | (date_hpg_reserve[\"holiday_flg\"]==1)).astype(int)\n",
    "        #date_air_reserve[\"holiday\"] = map(lambda a, b: 1 if a in [5, 6] or b == 1 else 0, date_air_reserve[\"weekday\"], date_air_reserve[\"holiday_flg\"])\n",
    "        #date_hpg_reserve[\"holiday\"] = map(lambda a, b: 1 if a in [5, 6] or b == 1 else 0, date_hpg_reserve[\"weekday\"], date_hpg_reserve[\"holiday_flg\"])\n",
    "\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_sum\", \"air_reserve_visitors_sum_weekday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_sum\", \"hpg_reserve_visitors_sum_weekday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_mean\", \"air_reserve_visitors_mean_weekday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_mean\", \"hpg_reserve_visitors_mean_weekday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_count\", \"air_reserve_visitors_count_weekday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"weekday\"],\"reserve_visitors_count\", \"hpg_reserve_visitors_count_weekday_mean_%s\"%i)\n",
    "\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_sum\", \"air_reserve_visitors_sum_holiday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_sum\", \"hpg_reserve_visitors_sum_holiday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_mean\", \"air_reserve_visitors_mean_holiday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_mean\", \"hpg_reserve_visitors_mean_holiday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_air_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_count\", \"air_reserve_visitors_count_holiday_mean_%s\"%i)\n",
    "        df_label=feat_mean(df_label,date_hpg_reserve,[\"air_store_id\",\"holiday\"],\"reserve_visitors_count\", \"hpg_reserve_visitors_count_holiday_mean_%s\"%i)\n",
    "\n",
    "\n",
    "    #月初月中月末\n",
    "    # Feiyang: 6. 把这两段的 mean 改成了 kernelMedian\n",
    "    df_label = feat_kernelMedian(df_label, df_train, [\"air_store_id\",\"day\",\"weekday\"], \"visitors\",PrEp, \"air_day_mean\")\n",
    "    df_label = feat_kernelMedian(df_label, df_train, [\"air_store_id\",\"day\",\"holiday\"], \"visitors\",PrEp, \"air_holiday_mean\")\n",
    "    for i in [21,35,63,140,280,350,420]:\n",
    "        df_select=df_train[df_train.day_gap>=-i].copy()\n",
    "\n",
    "        # Feiyang: 7. 给最重要的 visitors 这一列加上了新的特征: kernelMedian, median\n",
    "        df_label=feat_median(df_label, df_select, [\"air_store_id\"], \"visitors\", \"air_median_%s\"%i)\n",
    "        df_label=feat_mean(df_label,df_select,[\"air_store_id\"],\"visitors\", \"air_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_store_id\"],\"visitors\",PrEp,\"air_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_store_id\"],\"visitors\",\"air_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_store_id\"],\"visitors\",\"air_min_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"air_store_id\"],\"visitors\",\"air_std_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_store_id\"],\"visitors\",\"air_count_%s\"%i)\n",
    "\n",
    "        # Feiyang: 8. 把这几段的 mean 改成了 kernelMedian\n",
    "        #df_label=feat_mean(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\", \"air_week_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\",PrEp,\"air_week_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\",\"air_week_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\",\"air_week_min_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\",\"air_week_std_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\",\"air_week_count_%s\"%i)\n",
    "\n",
    "        # df_label=feat_mean(df_label,df_select,[\"air_store_id\",\"holiday\"],\"visitors\", \"air_holiday_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_store_id\",\"holiday\"],\"visitors\",PrEp,\"air_holiday_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_store_id\",\"holiday\"],\"visitors\",\"air_holiday_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_store_id\",\"holiday\"],\"visitors\",\"air_holiday_min_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_store_id\",\"holiday\"],\"visitors\",\"air_holiday_count_%s\"%i)\n",
    "\n",
    "        #df_label=feat_mean(df_label,df_select,[\"air_genre_name\",\"holiday\"],\"visitors\", \"air_genre_name_holiday_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_genre_name\",\"holiday\"],\"visitors\",PrEp,\"air_genre_name_holiday_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_genre_name\",\"holiday\"],\"visitors\",\"air_genre_name_holiday_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_genre_name\",\"holiday\"],\"visitors\",\"air_genre_name_holiday_min_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_genre_name\",\"holiday\"],\"visitors\",\"air_genre_name_holiday_count_%s\"%i)\n",
    "\n",
    "        #df_label=feat_mean(df_label,df_select,[\"air_genre_name\",\"weekday\"],\"visitors\", \"air_genre_name_weekday_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_genre_name\",\"weekday\"],\"visitors\",PrEp,\"air_genre_name_weekday_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_genre_name\",\"weekday\"],\"visitors\",\"air_genre_name_weekday_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_genre_name\",\"weekday\"],\"visitors\",\"air_genre_name_weekday_min_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_genre_name\",\"weekday\"],\"visitors\",\"air_genre_name_weekday_count_%s\"%i)\n",
    "\n",
    "        #df_label=feat_mean(df_label,df_select,[\"air_area_name\",\"holiday\"],\"visitors\", \"air_area_name_holiday_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_area_name\",\"holiday\"],\"visitors\",PrEp,\"air_area_name_holiday_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_area_name\",\"holiday\"],\"visitors\",\"air_area_name_holiday_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_area_name\",\"holiday\"],\"visitors\",\"air_area_name_holiday_min_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_area_name\",\"holiday\"],\"visitors\",\"air_area_name_holiday_count_%s\"%i)\n",
    "\n",
    "        #df_label=feat_mean(df_label,df_select,[\"air_area_name\",\"air_genre_name\",\"holiday\"],\"visitors\", \"air_area_genre_name_holiday_mean_%s\"%i)\n",
    "        df_label=feat_kernelMedian(df_label,df_select,[\"air_area_name\",\"air_genre_name\",\"holiday\"],\"visitors\",PrEp,\"air_area_genre_name_holiday_kermed_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"air_area_name\",\"air_genre_name\",\"holiday\"],\"visitors\",\"air_area_genre_name_holiday_max_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"air_area_name\",\"air_genre_name\",\"holiday\"],\"visitors\",\"air_area_genre_name_holiday_min_%s\"%i)\n",
    "        df_label=feat_count(df_label,df_select,[\"air_area_name\",\"air_genre_name\",\"holiday\"],\"visitors\",\"air_area_genre_name_holiday_count_%s\"%i)\n",
    "\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-04-09\n",
      "2017-03-26\n",
      "2017-03-12\n",
      "2017-02-26\n",
      "2017-02-12\n",
      "2017-01-29\n",
      "2017-01-15\n",
      "2017-01-01\n",
      "2016-12-18\n",
      "2016-12-04\n",
      "2016-11-20\n",
      "2016-11-06\n",
      "2016-10-23\n",
      "2016-10-09\n",
      "2016-09-25\n",
      "2016-09-11\n",
      "2016-08-28\n",
      "2016-08-14\n",
      "2016-07-31\n",
      "2016-07-17\n",
      "2016-07-03\n",
      "2016-06-19\n",
      "2016-06-05\n",
      "2016-05-22\n",
      "2016-05-08\n",
      "2016-04-24\n",
      "2016-04-10\n",
      "2016-03-27\n",
      "2016-03-13\n",
      "2016-02-28\n",
      "2017-04-23\n",
      "2017-03-26\n",
      "2017-02-26\n",
      "2017-01-29\n",
      "2017-01-01\n",
      "2016-12-04\n",
      "2016-11-06\n",
      "2016-10-09\n",
      "2016-09-11\n",
      "2016-08-14\n",
      "2016-07-17\n",
      "2016-06-19\n",
      "2016-05-22\n",
      "2016-04-24\n",
      "2016-03-27\n",
      "2016-02-28\n",
      "2017-04-23\n",
      "2017-03-12\n",
      "2017-01-29\n",
      "2016-12-18\n",
      "2016-11-06\n",
      "2016-09-25\n",
      "2016-08-14\n",
      "2016-07-03\n",
      "2016-05-22\n",
      "2016-04-10\n",
      "2016-02-28\n",
      "2017-04-23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f895c4cde0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mhpg_reserve\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"day_gap\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpg_reserve\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdate_gap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_begin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"air_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hpg_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visit_date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"day_gap\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mair_reserve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhpg_reserve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-5e0ef88a6c4c>\u001b[0m in \u001b[0;36mcreate_features\u001b[0;34m(df_label, df_train, df_air_reserve, df_hpg_reserve)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m#df_label=feat_mean(df_label,df_select,[\"air_store_id\",\"weekday\"],\"visitors\", \"air_week_mean_%s\"%i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mdf_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_kernelMedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"air_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visitors\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPrEp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"air_week_kermed_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mdf_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"air_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visitors\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"air_week_max_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mdf_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"air_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visitors\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"air_week_min_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mdf_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_select\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"air_store_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"visitors\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"air_week_std_%s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-433730f30832>\u001b[0m in \u001b[0;36mfeat_max\u001b[0;34m(df, df_feature, fe, value, name)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdf_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   8193\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8195\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   8196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8197\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mrindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         result_data = concatenate_block_managers(\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             b = make_block(\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0m_concatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     to_concat = [\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     to_concat = [\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     ]\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1757\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m     )\n\u001b[0;32m-> 1759\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### THIS takes a loong time.. \n",
    "\n",
    "for slip in [14,28,42]:   #you can add 21 35...\n",
    "    t2017 = date(2017, 4, 23)\n",
    "    nday=slip\n",
    "\n",
    "    #构造训练集\n",
    "    all_data=[]\n",
    "    for i in range(nday*1,nday*(420//nday+1),nday):  #windowsize==step\n",
    "        delta = timedelta(days=i)\n",
    "        t_begin=t2017 - delta\n",
    "        print(t_begin)\n",
    "        df_train[\"day_gap\"]=df_train[\"visit_date\"].apply(lambda x:date_gap(x,t_begin))\n",
    "        air_reserve[\"day_gap\"]=air_reserve[\"reserve_date\"].apply(lambda x:date_gap(x,t_begin))\n",
    "        hpg_reserve[\"day_gap\"]=hpg_reserve[\"reserve_date\"].apply(lambda x:date_gap(x,t_begin))\n",
    "        df_feature=df_train[df_train.day_gap<0].copy()\n",
    "        df_air_reserve=air_reserve[air_reserve.day_gap<0].copy()\n",
    "        df_hpg_reserve=hpg_reserve[hpg_reserve.day_gap<0].copy()\n",
    "        df_label=df_train[(df_train.day_gap>=0)&(df_train.day_gap<nday)][[\"air_store_id\",\"hpg_store_id\",\"visit_date\",\"day_gap\",\"visitors\"]].copy()\n",
    "        train_data_tmp=create_features(df_label,df_feature,df_air_reserve,df_hpg_reserve)\n",
    "        all_data.append(train_data_tmp)\n",
    "\n",
    "    train=pd.concat(all_data)\n",
    "\n",
    "\n",
    "    #构造线上测试集\n",
    "    t_begin=date(2017, 4, 23)\n",
    "    print(t_begin)\n",
    "    df_label=df_test.merge(store_id_relation,on=\"air_store_id\",how=\"left\")\n",
    "    df_label[\"day_gap\"]=df_label[\"visit_date\"].apply(lambda x:date_gap(x,t_begin))\n",
    "    df_train[\"day_gap\"]=df_train[\"visit_date\"].apply(lambda x:date_gap(x,t_begin))\n",
    "    air_reserve[\"day_gap\"] = air_reserve[\"reserve_date\"].apply(lambda x: date_gap(x, t_begin))\n",
    "    hpg_reserve[\"day_gap\"] = hpg_reserve[\"reserve_date\"].apply(lambda x: date_gap(x, t_begin))\n",
    "    df_label=df_label[[\"air_store_id\",\"hpg_store_id\",\"visit_date\",\"day_gap\"]].copy()\n",
    "    test=create_features(df_label,df_train,air_reserve,hpg_reserve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(clf,train_data,test_data,clf_name,class_num=1):\n",
    "        train=np.zeros((train_data.shape[0],class_num))\n",
    "        test=np.zeros((test_data.shape[0],class_num))\n",
    "        test_pre=np.empty((folds,test_data.shape[0],class_num))\n",
    "        cv_scores=[]\n",
    "        for i,(train_index,test_index) in enumerate(kf):\n",
    "            tr=train_data.iloc[train_index]\n",
    "            te=train_data.iloc[test_index]\n",
    "            #分别测试分数\n",
    "            te_1=te[te.day_gap<=6].copy()\n",
    "            te_2=te[te.day_gap>6].copy()\n",
    "            te_1_x=te_1.drop([\"visitors\"], axis=1)\n",
    "            te_2_x=te_2.drop([\"visitors\"], axis=1)\n",
    "            te_1_y=te_1[\"visitors\"].values\n",
    "            te_2_y=te_2[\"visitors\"].values\n",
    "            print(te_1.shape)\n",
    "            print(te_2.shape)\n",
    "\n",
    "            tr_x=tr.drop([\"visitors\"], axis=1)\n",
    "            tr_y=tr['visitors'].values\n",
    "            te_x=te.drop([\"visitors\"], axis=1)\n",
    "            te_y=te['visitors'].values\n",
    "\n",
    "            weight_train=weight_df.iloc[train_index]\n",
    "            weight_test=weight_df.iloc[test_index]\n",
    "\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y,weight=weight_train[\"weight\"])\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y,weight=weight_test[\"weight\"])\n",
    "\n",
    "            params = {\n",
    "                # Feiyang: 10. 把 7 改成了 8\n",
    "                'num_leaves': 2 ** 8 - 1,\n",
    "                'objective': 'regression_l2',\n",
    "                # Feiyang: 11. 把 8 改成了 9\n",
    "                'max_depth': 9,\n",
    "                'min_data_in_leaf': 50,\n",
    "                # Feiyang: 12. 把 0.01 改成了 0.007 并同时改了下面的 Num_round 和 early_stopping_rounds\n",
    "                'learning_rate': 0.007,\n",
    "                'feature_fraction': 0.6,\n",
    "                # Feiyang: 13. 把 0.75 改成了 0.8\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 1,\n",
    "                'metric': 'rmse',\n",
    "                'num_threads': 4,\n",
    "                'seed': 2018,\n",
    "            }\n",
    "\n",
    "            num_round = 6000\n",
    "            early_stopping_rounds = 500\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape((te_x.shape[0],1))\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_data, num_iteration=model.best_iteration).reshape((test_data.shape[0],1))\n",
    "                pre_1=model.predict(te_1_x,num_iteration=model.best_iteration).reshape((te_1_x.shape[0],1))\n",
    "                pre_2=model.predict(te_2_x,num_iteration=model.best_iteration).reshape((te_2_x.shape[0],1))\n",
    "                cv_scores.append((mean_squared_error(te_y, pre)**0.5,mean_squared_error(te_1_y, pre_1)**0.5,mean_squared_error(te_2_y, pre_2)**0.5))\n",
    "\n",
    "            print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "        test[:]=test_pre.mean(axis=0)\n",
    "        print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "        print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "\n",
    "        score_split=(str(round(np.mean([i[0] for i in cv_scores]),6)),str(round(np.mean([i[1] for i in cv_scores]),6)),str(round(np.mean([i[2] for i in cv_scores]),6)))\n",
    "        with open(\"score_cv.txt\", \"a\") as f:\n",
    "            f.write(\"%s now score is:\" % clf_name + str(cv_scores) + \"\\n\")\n",
    "            f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "            f.write(\"score_split:\"+str(score_split)+\"\\n\")\n",
    "\n",
    "        return train.reshape(-1,class_num),test.reshape(-1,class_num),score_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb(train, valid):\n",
    "        xgb_train, xgb_test,cv_scores = stacking(lightgbm,train,valid,\"lgb\")\n",
    "        return xgb_train, xgb_test,cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "    from sklearn.cross_validation import KFold\n",
    "    folds = 5\n",
    "    seed = 2018\n",
    "\n",
    "    #生成数据\n",
    "    train_data = train.drop([\"air_store_id\",\"hpg_store_id\",\"visit_date\"], axis=1)\n",
    "    test_data = test.drop([\"air_store_id\",\"hpg_store_id\",\"visit_date\"], axis=1)\n",
    "\n",
    "    weight_df=train[[\"day_gap\"]].copy()\n",
    "    weight_df[\"weight\"]=weight_df[\"day_gap\"].apply(lambda x: 1 if x<=6 else 1)\n",
    "\n",
    "    kf = KFold(train.shape[0], n_folds=folds, shuffle=True, random_state=seed)\n",
    "    lgb_train, lgb_test,m=lgb(train_data,test_data)\n",
    "\n",
    "    #生成线下\n",
    "    train[\"visitors_pre\"]=lgb_train\n",
    "    score_result=mean_squared_error(train[\"visitors\"], train[\"visitors_pre\"])**0.5\n",
    "    train[\"visitors\"] = np.clip(np.expm1(train[\"visitors\"]), 0, 1000)\n",
    "    train[\"visitors_pre\"] = np.clip(np.expm1(train[\"visitors_pre\"]), 0, 1000)\n",
    "    train[[\"air_store_id\",\"visit_date\",\"visitors\",\"visitors_pre\"]].to_csv(\"../offline/offline_cv_%s_%s_%s.csv\"%m,index=None)\n",
    "    with open(\"score_cv.txt\", \"a\") as f:\n",
    "        f.write(\"result score is:\" + str(score_result) + \"\\n\")\n",
    "    #生成提交\n",
    "    df_test[\"visitors\"]=lgb_test\n",
    "    df_test[\"visitors\"] = np.clip(np.expm1(df_test[\"visitors\"]), 0, 1000)\n",
    "    df_test[[\"id\",\"visitors\"]].to_csv(\"sub_plantsgo_cv_%s_weight_%s_%s_%s.csv\"%(slip,m[0],m[1],m[2]),index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
